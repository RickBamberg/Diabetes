
# Passo 01
# Carregando as bibliotecas
#!pip install .....
-----------------------------------------
# Passo 02
# A primeira coisa que temos que fazer é importar os pacotes que iremos utilizar.
# Obs.: Pacotes do Python são conjuntos de funcionalidades disponíveis da ferramenta.

#Pandas: Possui inúmeras funções e comandos para importar arquivos, analisar dados, tratar dados, etc.
import pandas as pd

# Pandasgui: Descrição graficas do dataframe
from pandasgui import show
import pandas as pd

#Matplotlib: Possui uma série de funções e comandos para exibição de gráficos
import matplotlib.pyplot as plt

#Seaborn: Possui uma série de funções e comandos para exibição de gráficos (Visualizações mais robustas do que o Matplotlib)
import seaborn as sns

#Numpy: Possui uma série de funções e comandos para trabalharmos com números de forma em geral(formatação, calculos, etc)
import numpy as np

# Biblioteca para fazer a PADRONIZAÇÃO
from sklearn.preprocessing import StandardScaler

# Biblioteca para fazer a PADRONIZAÇÃP ROBUST
from sklearn.preprocessing import RobustScaler

# Biblioteca para fazer a NORMALIZAÇÃO
from sklearn.preprocessing import MinMaxScaler

# Biblioteca para construção de modelos compostos com transformadores.
from sklearn.compose import ColumnTransformer

# Biblioteca para Balanciamento de dados
from imblearn.over_sampling import SMOTE

# Bilioteca para separação de dados em treino e teste
from sklearn.model_selection import train_test_split

# Biblioteca para calcular a acuracia do modelo
from sklearn.metrics import accuracy_score

# Algoritmo KNN
from sklearn.neighbors import KNeighborsClassifier

# Algoritmo SVM
from sklearn import svm

# Algoritmo SVC
from sklearn.svm import SVC

# Algoritmo LogisticRregression
from sklearn.linear_model import LogisticRegression

# Algoritmo RandomForestClassifier
from sklearn.ensemble import RandomForestClassifier

# Algoritmo DecisionTreeClassifier
from sklearn.tree import DecisionTreeClassifier

# Algoritmo accurcy_score
from sklearn.metrics import classification_report, confusion_matrix

# Biblioteca imblearn de pipeline com Função de Conveniência
from imblearn.pipeline import make_pipeline

#Warnings: Possui detalhes sobre os avisos e alertas que aparecem, porém podemos utiliza-lo também para que os alertas de
#futuras atualizações e metodos depreciados não sejam exibidos
import warnings
warnings.filterwarnings("ignore") 
pd.set_option('display.max_columns', None)
pd.set_option('display.max_rows', None)
#pd.options.display.float_format = '{:.2f}'.format

# Locale: Utilizado para formatação de datas, valores, dias de acordo com a região que queremos.
import locale

# Comandos do sistema operacional
import os
-----------------------------------------------------
#Comando utilizado para carregar o arquivo e armazena-lo como um DataFrame do Pandas
#Um DataFrame do Pandas é como se fosse uma planilha do Excel, onde podemos tratar linhas e colunas.

nome = 'Diabetes.csv'
df = pd.read_csv(nome)
-----------------------------------------------------
# Ver as primeiras linhas

print("Primeiras linhas:")
display(df.head())
----------------------------------------------------
# Ver as últimas linhas

print("\nÚltimas linhas:")
display(df.tail())
----------------------------------------------------
# Obter informações gerais (tipos de dados, contagem de não nulos, uso de memória)

print("\nInformações do DataFrame:")
df.info()
----------------------------------------------------
# Obter as dimensões (linhas, colunas)

print("\nShape (Linhas, Colunas):")
print(df.shape)
----------------------------------------------------
# Listar os nomes das colunas

print("\nColunas:")
print(df.columns)
----------------------------------------------------
# Avaliar se alguma variável possui valor nulo ou chamado de valores missing ou "NAN" (Not Available)

df.isnull().sum()
---------------------------------------------------
# Verificando o número de valores únicos

df.nunique()
---------------------------------------------------
# Mostrar as linhas duplicadas

display(df[df.duplicated(keep=False)]) # keep=False mostra todas as ocorrências de duplicatas
---------------------------------------------------
# Visualizando algumas medidas estatísticas

df.describe()
--------------------------------------------------
# Renomear uma coluna

df.rename(columns={"Pregnancies" : "Gravidez"}, inplace=True)
df.rename(columns={"Glucose" : "Glicose"}, inplace=True)	
df.rename(columns={"BloodPressure" : "Pressão arterial"}, inplace=True)
df.rename(columns={"SkinThickness" : "Espessura da pele"}, inplace=True)
df.rename(columns={"Insulin" : "Insulina"}, inplace=True)
df.rename(columns={"BMI" : "IMC"}, inplace=True)
df.rename(columns={"DiabetesPedigreeFunction" : "Diabetes Descendente"}, inplace=True) 
df.rename(columns={"Age" : "Idade"}, inplace=True)
df.rename(columns={"Outcome" : "Resultado"}, inplace=True)
df.info()
------------------------------------------------------
# Gerando um BoxPlot de todas variaveis
plt.figure(figsize=(16,6))
ax = sns.boxplot(data = df)
------------------------------------------------------
# Tratamento das colunas com valores zerados.
# Colocar a mediana sem os valosres zerados nos campos que estão com valor zeros.

# Lista das colunas onde zero é implausível e precisa ser tratado
colunas_para_imputar = ['Glicose', 'Pressão arterial', 'Espessura da pele', 'Insulina', 'IMC']
print("Valores médios ANTES da imputação (incluindo zeros onde aplicável):")
print(df[colunas_para_imputar].mean()) # Apenas para comparação depois

print("\nAntes - estatísticas descritivas (colunas selecionadas):")
print(df[colunas_para_imputar].describe())

print("-" * 30)

for coluna in colunas_para_imputar:
    # 1. Seleciona a coluna
    series_coluna = df[coluna]

    # 2. Calcula a mediana IGNORANDO os zeros
    #    (Criamos uma cópia temporária sem zeros para calcular a mediana)
    mediana_coluna = series_coluna[series_coluna > 0].median()

    # Verificação se a mediana foi calculada (caso todos os valores > 0 fossem NaN ou a coluna só tivesse 0)
    if pd.isna(mediana_coluna):
        # Se não puder calcular a mediana (ex: só tem zeros),
        # você pode usar a média geral (menos ideal) ou outra estratégia.
        # Mas para estas colunas, é quase certo que haverá valores > 0.
        print(f"Aviso: Não foi possível calcular a mediana não-zero para '{coluna}'. Pulando imputação.")
        continue # Pula para a próxima coluna

    print(f"Coluna: '{coluna}' - Mediana (sem zeros): {mediana_coluna:.2f}")

    # 3. Faz o replace dos zeros com a mediana calculada para ESSA coluna
    #    Usamos .loc para evitar SettingWithCopyWarning e garantir a modificação no df original
    df.loc[df[coluna] == 0, coluna] = mediana_coluna

    # Alternativa (funciona também, mas .loc é preferido):
    # df[coluna] = df[coluna].replace(0, mediana_coluna)

print("-" * 30)
print("Imputação concluída.")

# 4. Verificação Opcional: Contar zeros restantes e ver novas estatísticas
print("\nZeros restantes nas colunas imputadas:")
print((df[colunas_para_imputar] == 0).sum()) # Deve ser tudo zero

print("\nValores médios DEPOIS da imputação:")
print(df[colunas_para_imputar].mean()) # Compare com antes

print("\nNovas estatísticas descritivas (colunas selecionadas):")
print(df[colunas_para_imputar].describe())
--------------------------------------------------------------
# Gerando um BoxPlot de todas variaveis
plt.figure(figsize=(16,6))
ax = sns.boxplot(data = df)
--------------------------------------------------------------
# Verificando se a variável resposta está balanceada
target_count = df.Resultado.value_counts()
target_count
--------------------------------------------------------------
# Distribuição dos resultado antes do balanceamentos
# Tamanho da figura (opcional)
plt.figure(figsize=(6,4))

# Título
plt.title("Distribuição do Resultado (0 = Não, 1 = Sim)")

# Gráfico de contagem
sns.countplot(x='Resultado', data=df, palette='pastel')

# Rótulo dos eixos
plt.xlabel("Tem Diabetes")
plt.ylabel("Quantidade")

# Mostrar os valores no topo das barras (opcional)
for p in plt.gca().patches:
    plt.gca().annotate(f'{int(p.get_height())}', 
                       (p.get_x() + p.get_width() / 2., p.get_height()), 
                       ha='center', va='bottom')

plt.tight_layout()
plt.show()
-------------------------------------------------------------
# DataFrame Original (sem padronização)
df_original = df.copy()

# Separação do Target dos Features
X = df.drop('Resultado', axis=1) # Features
Y = df['Resultado']              # Target
print('Separação do Target dos Features')
-------------------------------------------------------------
# Separa TODO o conjunto (X e Y juntos, mas mantendo a correspondência)
# em 70% para treino e 30% para teste.
X_train_df, X_test_df, Y_train_df, Y_test_df = train_test_split(
    X, Y,
    test_size=0.3,   # 30% dos dados vão para o conjunto de teste
    random_state=42, # Para reprodutibilidade do embaralhamento
    stratify=Y       # IMPORTANTE para classificação: mantém a proporção das classes em treino/teste
)
print('Separação do Treino e Teste 70/30')
--------------------------------------------------------------
#from matplotlib import pyplot as plt
plt.figure(figsize=(16,6))
ax = sns.boxplot(data = df_original)
-------------------------------------------------------------
# Padrozinação StandardScaler
scaler_std = StandardScaler()
scaler_std.fit(X_train_df)
X_train_df_pad = scaler_std.transform(X_train_df)
X_test_df_pad = scaler_std.transform(X_test_df)
print('Transformação do DataFrame Padronizado')
-------------------------------------------------------------
# Balanceamento do X_train_df_pad -- SMOTE
smote = SMOTE(random_state=42)
X_train_df_pad_bal, Y_train_df_pad_bal = smote.fit_resample(X_train_df_pad, Y_train_df)
print("\nDistribuição no Treino Após SMOTE:")
print(pd.Series(Y_train_df_pad_bal).value_counts())
print('Balanceamento do DataFrame Padronizado')
-------------------------------------------------------------
# Monta Dataframe df_padronizado
df_padronizado = pd.DataFrame(scaler_std.transform(X), columns=X.columns)
df_padronizado['Resultado'] = Y.values
df_padronizado.head()
-------------------------------------------------------------
#from matplotlib import pyplot as plt
plt.figure(figsize=(16,6))
ax = sns.boxplot(data = df_padronizado)
-------------------------------------------------------------
# Normatização MinMaxScaler
scaler_minmax = MinMaxScaler()
scaler_minmax.fit(X_train_df)
X_train_df_norm = scaler_minmax.transform(X_train_df)
X_test_df_norm = scaler_minmax.transform(X_test_df)
print('Transformação do DataFrame Normatizado')
------------------------------------------------------------
# Balanceamento do X_train_df_norm -- SMOTE
smote = SMOTE(random_state=42)
X_train_df_norm_bal, Y_train_df_norm_bal = smote.fit_resample(X_train_df_norm, Y_train_df)
print("\nDistribuição no Treino Após SMOTE:")
print(pd.Series(Y_train_df_norm_bal).value_counts())
print('Balanceamento do DataFrame Padronizado')
------------------------------------------------------------
# Monta DataFrame df_normalizado
df_normalizado = pd.DataFrame(scaler_minmax.transform(X), columns=X.columns)
df_normalizado['Resultado'] = Y.values
df_normalizado.head()
------------------------------------------------------------
#from matplotlib import pyplot as plt
plt.figure(figsize=(16,6))
ax = sns.boxplot(data = df_normalizado)
------------------------------------------------------------
# Padronização Mista
# Defina as colunas para cada tipo de transformação (usando os nomes ORIGINAIS)
colunas_robust = ['Insulina', 'Espessura da pele', 'IMC']
colunas_standard = ['Glicose', 'Pressão arterial']
colunas_passthrough = ['Gravidez', 'Idade']

# Aplique as transformações SEM renomear ainda
transformer = ColumnTransformer(
    transformers=[
        ('robust', RobustScaler(), colunas_robust),
        ('standard', StandardScaler(), colunas_standard),
        ('pass', 'passthrough', colunas_passthrough)
    ],
    remainder='passthrough'
)

transformer.fit(X_train_df)

X_train_df_misto = transformer.transform(X_train_df)
X_test_df_misto = transformer.transform(X_test_df)
print('Transformação do DataFrame Misto')
-------------------------------------------------------------
# Balanceamento do X_train_df_misto -- SMOTE
smote = SMOTE(random_state=42)
X_train_df_misto_bal, Y_train_df_misto_bal = smote.fit_resample(X_train_df_misto, Y_train_df)
print("\nDistribuição no Treino Após SMOTE:")
print(pd.Series(Y_train_df_misto_bal).value_counts())
print('Balanceamento do DataFrame Misto')
-------------------------------------------------------------
# Monta DataFrame df_misto
df_misto = pd.DataFrame(transformer.transform(X), columns=transformer.get_feature_names_out())
df_misto['Resultado'] = Y.values
df_misto.head()
------------------------------------------------------------
#from matplotlib import pyplot as plt
plt.figure(figsize=(16,6))
ax = sns.boxplot(data = df_misto)
------------------------------------------------------------
# Suponha que 'df_original' seja seu DataFrame completo e 'target' a coluna alvo
# Pode ser usado o df_padronizado, df_narmatizado, df_misto
X = df_original.drop('Resultado', axis=1)
y = df_original['Resultado']
-------------------------------------------------------------
# Divisão em Dados de Treino e Teste.
X_treino, X_teste, y_treino, y_teste = train_test_split(X, y, test_size = 0.3, random_state = 42)
--------------------------------------------------------------
# 1. Regressão Logística
modelo_lr = LogisticRegression()
modelo_lr.fit(X_treino, y_treino)
y_pred_lr = modelo_lr.predict(X_teste)
--------------------------------------------------------------
# 2. Random Forest
modelo_rf = RandomForestClassifier()
modelo_rf.fit(X_treino, y_treino)
y_pred_rf = modelo_rf.predict(X_teste)
-------------------------------------------------------------
# 3. Support Vector Machine
modelo_svm = SVC()
modelo_svm.fit(X_treino, y_treino)
y_pred_svm = modelo_svm.predict(X_teste)
-------------------------------------------------------------
# 4. XGBClassifier
from xgboost import XGBClassifier
modelo_xgb = XGBClassifier()
modelo_xgb.fit(X_treino, y_treino)
y_pred_xgb = modelo_xgb.predict(X_teste)
------------------------------------------------------------
# 4. Árvore de Decisão
modelo_dt = DecisionTreeClassifier()
modelo_dt.fit(X_treino, y_treino)
y_pred_dt = modelo_dt.predict(X_teste)
------------------------------------------------------------
# Avaliação dos modelos
modelos = {
    "Regressão Logística": y_pred_lr,
    "Random Forest": y_pred_rf,
    "SVM": y_pred_svm,
    "XGBClassifier": y_pred_xgb,
    "Árvore de Decisão": y_pred_dt
}
------------------------------------------------------------
for nome, y_pred in modelos.items():
    print(f"\nModelo: {nome}")
    print("Acurácia:", accuracy_score(y_teste, y_pred))
    print("Matriz de Confusão:\n", confusion_matrix(y_teste, y_pred))
    print("Relatório de Classificação:\n", classification_report(y_teste, y_pred))
--------------------------------------------------------------
# Dicionário de pré-processadores
preprocessadores = {
    'Original': None,
    'Padronizado': StandardScaler(),
    'Normalizado': MinMaxScaler(),
    'Misto': ColumnTransformer([
        ('robust', RobustScaler(), ['Insulina', 'Espessura da pele', 'IMC']),
        ('standard', StandardScaler(), ['Glicose', 'Pressão arterial']),
        ('pass', 'passthrough', ['Gravidez', 'Idade'])
    ])
}

resultados = []

for prep_name, prep in preprocessadores.items():
    X = df.drop('Resultado', axis=1)
    y = df['Resultado']
    
    # Split inicial (mantendo proporção original)
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.3, random_state=42, stratify=y)
    
    # Pipeline com balanceamento APENAS no treino
    pipeline = make_pipeline(
        prep if prep else 'passthrough',  # Pré-processamento
        SMOTE(random_state=42),          # Balanceamento
        RandomForestClassifier()         # Modelo
    )
    
    pipeline.fit(X_train, y_train)
    y_pred = pipeline.predict(X_test)
    
    # Armazenar resultados
    report = classification_report(y_test, y_pred, output_dict=True)
    resultados.append({
        'Pré-processamento': prep_name,
        'Acurácia': report['accuracy'],
        'Precision_0': report['0']['precision'],
        'Recall_0': report['0']['recall'],
        'Precision_1': report['1']['precision'],
        'Recall_1': report['1']['recall']
    })

# Exibir resultados
pd.DataFrame(resultados)
-------------------------------------------------------------------


seu-projeto-diabetes/
├── notebooks/             # <--- NOVA PASTA AQUI
│   └── diabetes_analysis_and_training.ipynb  # <--- SEU NOTEBOOK AQUI
│
├── app.py                 # Script principal Flask
├── model/
│   ├── model.pkl          # Modelo treinado
│   └── preprocessor.pkl   # Pré-processador salvo
├── templates/
│   ├── index.html
│   ├── result.html
│   ├── batch_upload.html
│   └── batch_results.html
├── static/                # (Opcional) CSS, JS, Imagens
│   └── css/
│       └── style.css
├── data/                  # (Opcional) Exemplo de CSV
│   └── exemplo_batch.csv
│
├── requirements.txt       # Dependências Python (para Flask E Notebook)
├── .gitignore             # Ignorar arquivos/pastas
└── README.md              # Documentação principal


app.py

import joblib
import pandas as pd
from flask import Flask, request, render_template, redirect, url_for
import numpy as np # Se necessário para manipulação

app = Flask(__name__)

# Carregar o modelo e o pré-processador (faça isso fora das rotas para eficiência)
try:
    model = joblib.load('model/model.pkl')
    preprocessor = joblib.load('model/preprocessor.pkl')
    print("Modelo e pré-processador carregados com sucesso.")
except FileNotFoundError:
    print("Erro: Arquivos model.pkl ou preprocessor.pkl não encontrados na pasta 'model/'.")
    # Tratar o erro adequadamente (ex: sair ou definir como None e checar nas rotas)
    model = None
    preprocessor = None
except Exception as e:
    print(f"Erro ao carregar modelo/pré-processador: {e}")
    model = None
    preprocessor = None

# ... (suas rotas Flask) ...

if __name__ == '__main__':
    app.run(debug=True) # debug=True para desenvolvimento, False para "produção"
	
----------------------------------------------------------------------------
Rotas Claras: Mantenha suas rotas (@app.route('/'), 
@app.route('/predict', methods=['POST']), @app.route('/batch_predict'), etc.) bem definidas e com nomes lógicos.
Pré-processamento ANTES da Predição:
Entrada Única: Quando receber os dados do formulário, organize-os na mesma 
ordem de colunas que o modelo espera (provavelmente como um DataFrame de 
uma linha ou um array NumPy). Aplique o preprocessor.transform() nesses dados ANTES de passá-los para model.predict().
Entrada em Lote (CSV): Leia o CSV com Pandas. Certifique-se de que as colunas estão corretas. Aplique o preprocessor.transform() 
no DataFrame lido ANTES de usar model.predict().
Tratamento de Erros: Adicione blocos try...except para lidar com entradas inválidas (ex: texto em campos numéricos), 
arquivos CSV mal formatados, ou falhas no carregamento dos modelos. Retorne mensagens de erro úteis para o usuário no HTML.

3. model/:

Consistência: Certifique-se de que o preprocessor.pkl salvo seja exatamente o 
mesmo objeto (StandardScaler, ColumnTransformer, etc.) que foi ajustado (fit) 
nos dados de treino usados para treinar o model.pkl. Se você treinou o modelo com 
dados padronizados, o preprocessor.pkl deve ser o StandardScaler ajustado. Se usou o ColumnTransformer, 
salve esse objeto. A falta dessa consistência é uma causa comum de erros.

Salvar/Carregar: joblib é geralmente preferido sobre pickle para objetos Scikit-learn, 
pois pode ser mais eficiente com arrays NumPy grandes.

-------------------------------------------------------
# Para salvar (no seu notebook ou script de treinamento)
import joblib
joblib.dump(seu_modelo_rf, 'model/model.pkl')
joblib.dump(seu_objeto_preprocessador, 'model/preprocessor.pkl') # Ex: scaler, transformer

# Para carregar (no app.py)
model = joblib.load('model/model.pkl')
preprocessor = joblib.load('model/preprocessor.pkl')

--------------------------------------------------------------------
4. templates/:

Mantenha seus HTMLs limpos e focados na interface do usuário.
Use formulários (<form>) para enviar os dados para as rotas POST do Flask.
Use a sintaxe do Jinja2 ({{ variavel }}) 
para exibir os resultados passados pelo Flask (render_template).

5. requirements.txt:
Essencial para que outros (ou você mesmo em outro ambiente) 
possam instalar as dependências corretas. Gere-o no seu ambiente virtual ativado:

pip freeze > requirements.txt

--------------------------------------------------------

Inclua Flask, pandas, scikit-learn, imblearn (se o pré-processador veio de lá ou se você o usa no Flask), numpy, joblib, xgboost (se usou), etc.

6. .gitignore:

Crie este arquivo para evitar que arquivos desnecessários ou sensíveis sejam enviados para o GitHub. Um bom .gitignore para Python/Flask incluiria:

# Byte-compiled / optimized / DLL files
__pycache__/
*.py[cod]
*$py.class

# C extensions
*.so

# Distribution / packaging
.Python
build/
develop-eggs/
dist/
downloads/
eggs/
.eggs/
lib/
lib64/
parts/
sdist/
var/
wheels/
pip-wheel-metadata/
share/python-wheels/
*.egg-info/
.installed.cfg
*.egg
MANIFEST

# PyInstaller
# Usually these files are written by a python script from a template
# before PyInstaller builds the exe, so as to inject date/other infos into it.
*.manifest
*.spec

# Installer logs
pip-log.txt
pip-delete-this-directory.txt

# Unit test / coverage reports
htmlcov/
.tox/
.nox/
.coverage
.coverage.*
.cache
nosetests.xml
coverage.xml
*.cover
*.py,cover
.hypothesis/
.pytest_cache/

# Environments
.env
.venv
env/
venv/
ENV/
env.bak/
venv.bak/

# Spyder project settings
.spyderproject
.spyproject

# Rope project settings
.ropeproject

# mkdocs documentation
/site

# mypy
.mypy_cache/
.dmypy.json
dmypy.json

# Jupyter Notebook
.ipynb_checkpoints

# IDE settings
.idea/
.vscode/

# Flask specific
instance/ # Pasta de instância do Flask

# Other
*.log
local_settings.py

---------------------------------------------------

7. README.md (Foco na Aplicação):
Este README é diferente do README do notebook. Ele deve explicar:
O que a aplicação faz: Descreva brevemente os dois modos (predição única e em lote).
Como configurar e executar:
Clonar o repositório (git clone ...).
Criar e ativar um ambiente virtual (python -m venv venv, source venv/bin/activate ou venv\Scripts\activate).
Instalar as dependências (pip install -r requirements.txt).
Como executar a aplicação (python app.py).
Informar o endereço local onde a aplicação estará rodando (ex: http://127.0.0.1:5000).
Como usar:
Instruções para a interface web (preencher campos, clicar botão).
Instruções para o upload em lote (formato esperado do CSV, quais colunas, exemplo).
Tecnologias Usadas: Liste as principais bibliotecas (Flask, Scikit-learn, Pandas, etc.).
(Opcional) Link para o Notebook: Se o notebook de análise/treinamento também estiver no repositório (ou em outro), coloque um link para ele.
(Opcional) Screenshots: Imagens da interface web podem ser muito úteis.
Resumo das Ações:
Crie a estrutura de pastas sugerida.
Mova seus arquivos (.py, .pkl, .html) para as pastas corretas.
Verifique se o carregamento do modelo e pré-processador está correto no app.py.
Garanta que o pré-processamento é aplicado antes da predição nas suas rotas Flask.
Gere o requirements.txt.
Crie ou copie um bom .gitignore.
Escreva um README.md detalhado focado na aplicação Flask.
Com essa organização, seu projeto ficará muito profissional e fácil de entender e usar por outras pessoas no GitHub! Novamente, parabéns pelo progresso! Se tiver dúvidas ao organizar, pode perguntar.
----------------------------------------------------------------------------------

notebook

Passos e Considerações:
Criar a Pasta notebooks/: Crie uma pasta chamada notebooks (ou analysis, development, training_scripts
 - escolha um nome que faça sentido para você) na raiz do seu projeto.
Mover o Notebook: Coloque seu arquivo .ipynb dentro dessa pasta notebooks/. 
Dê a ele um nome descritivo, como diabetes_analysis_and_training.ipynb ou similar.
Caminhos no Notebook: Verifique se os caminhos dentro do seu notebook para carregar dados 
(ex: pd.read_csv('Diabetes.csv')) ou salvar os modelos (joblib.dump(model, 'model/model.pkl')) 
estão corretos em relação à estrutura raiz do projeto.
Se você executar o Jupyter Notebook a partir da raiz do projeto, os caminhos como 
'data/exemplo_batch.csv' ou 'model/model.pkl' devem funcionar diretamente.
Se você navegar para a pasta notebooks/ e iniciar o Jupyter de lá, talvez precise ajustar 
os caminhos para referenciar a pasta pai (ex: pd.read_csv('../data/Diabetes.csv'), joblib.dump(model, '../model/model.pkl')). 
A primeira abordagem (executar da raiz) é geralmente mais simples de gerenciar.
requirements.txt Abrangente: Certifique-se de que seu requirements.txt inclua todas as bibliotecas necessárias 
tanto para a execução do Flask (Flask, joblib, etc.) quanto para a execução do notebook 
(pandas, numpy, scikit-learn, imblearn, matplotlib, seaborn, jupyter - opcional, mas bom para quem for rodar).
Atualizar o README.md: No seu README.md principal, adicione uma seção ou menção sobre a pasta notebooks/:
Explique que esta pasta contém o Jupyter Notebook detalhando a análise dos dados, o pré-processamento, 
o treinamento do modelo e a avaliação que levaram à criação dos arquivos .pkl usados pela aplicação Flask.
Você pode até colocar um link direto para o arquivo do notebook dentro do repositório no GitHub. Exemplo:

## Notebook de Análise e Treinamento

O processo completo de análise exploratória dos dados, engenharia de features, treinamento e avaliação do modelo Random Forest pode ser encontrado no seguinte notebook:
[notebooks/diabetes_analysis_and_training.ipynb](notebooks/diabetes_analysis_and_training.ipynb)

Este notebook também mostra como os arquivos `model/model.pkl` e `model/preprocessor.pkl` foram gerados.

.gitignore: Confirme que .ipynb_checkpoints está no seu .gitignore para não versionar os checkpoints automáticos do Jupyter.
Ao fazer isso, você terá um repositório completo e bem organizado, mostrando não apenas a aplicação final (Flask), 
mas também todo o trabalho de ciência de dados que a fundamentou. Isso é muito valorizado!



